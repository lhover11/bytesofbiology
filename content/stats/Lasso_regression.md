+++
title = 'Lasso Regression'
date = 2024-03-07T19:15:13-05:00
draft = false
comments = true
+++

What is lasso regression?  Lasso regression stands for Least Absolute Shrinkage and Selection Operator and is a type of linear regression that can be used to predict an outcome or target variable based on multiple input features

Lasso regression adds a penalty term to the traditional linear regression model, which leads to more sparse models as coefficients for some features are pushed to zero and effectively removed from the model

Why use lasso regression?
1. It's useful for feature selection, as the penalty term will push some coefficients to 0
2. Similary, it's useful when you have features that are highly correlated with each other, as again, coefficient terms can be pushed to 0 and features can be eliminated from the model
3. It can help with overfitting

How does it do all of this?  Lasso adds a penalty or L1 regularization term (called alpha or lambda) which can shrink the coefficients to 0
As alpha gets larger, more of the coefficients will be shrunk to 0 and are eliminated from the model
As alpha gets smaller, more of the coefficients will be non-zero
An alpha = 0 will give you the same model as linear regression as all features are used in the model (we can test this below)

Let's try it and compare lasso with regular linear regression


```python
# Load packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge, RidgeCV, Lasso
from sklearn.linear_model import LassoCV
import matplotlib.pyplot as plt
```


```python
# Next I will read in my data
# This data was generated by asking chatgpt to give me a dataset where 5 features are important and the rest are noise so we can compare the difference between lieanr and lasso regression with feature selection

data = pd.read_csv("lasso_regression_dataset.csv")
data.head()

# (dataset is available in the Files tab)
```

So here we can see we have 100 features, 1 target variable (our y)


```python
# Next we'll split our data into training and testing data using 70% as training data and 30% as testing data
features = data.columns[0:100] # columns 0-100 are our features
target = data.columns[-1] # last column

#X and y values
X = data[features].values
y = data[target].values

# Split data into testing and training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)
# test_size = 0.3 will make the testing data set 30% of our total input data
# setting random_state to 99 will keep the results consistent if we re-run our script

print(X_train.shape)
print(X_test.shape)
```

    (70, 100)
    (30, 100)



```python
# First let's run a linear regression

# Model
linear = LinearRegression()

# fit the model using the training data
linear.fit(X_train, y_train)

# Get our predicted values using our testing data 
pred1 = linear.predict(X_test)

# Get our actual values from the testing data
truth = y_test

train_score = linear.score(X_train, y_train)
test_score = linear.score(X_test, y_test)

print(train_score)
print(test_score)
```

    1.0
    0.2659386480244982


According to the sklearn documentation, the score here represents the coefficient of determination (R^2), or how well the model is explaining the variaibility in the target
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html

Using our TESTING data, the model can predict about 27% of the variability in the target variable
However, you can see how you might be led astray if you had only looked at the training data, as the model perfectly predicts the training data

Let's compare the model in the testing and the training data


```python
# get our predicted outcome for the training data
train_pred = linear.predict(X_train)
```


```python
# Now we plot the predicted vs the real values for the TRAINING data
plt.scatter(train_pred, y_train, color = "mediumvioletred")
plt.xlabel('Real Target')
plt.ylabel('Predicted Target')
plt.title("Training data: Predicted vs Real Target")

# Add diagonal dashed line
plt.plot([-6, 6], [-6, 6], '--', color='gray')

plt.show()
```


    
![png](../images/Lasso_regression_9_0.png)
    



```python
# Now we plot the predicted vs the real values for the TESTING data
plt.scatter(pred1, truth, color = "teal")

plt.xlabel('Real Target')
plt.ylabel('Predicted Target')
plt.title("Testing data: Predicted vs Real Target")
plt.xlim(-6, 6)
plt.ylim(-6, 6)

# Add diagonal dashed line
plt.plot([-6, 6], [-6, 6], '--', color='gray')

plt.show()
```


    
![png](../images/Lasso_regression_10_0.png)
    


As you can see, these plots are quite different, showing our model was overfit to the training data and really doesn't do a great job of predicting the target using other input data.

This also highlights the importance of splitting your data into testing and training datasets when building models as your model may trick you as it looks like it works really well, but its actually just overfit to the input data

OK, back to the main point of this post, Lasso regression
So let's see if Lasso regression can do better


```python
# We'll use cross validation to find the best alpha parameter
lasso = LassoCV(random_state=99).fit(X_train, y_train)

# Get our predicted values using our testing data 
pred2 = lasso.predict(X_test)

#score
print(lasso.score(X_train, y_train))
print(lasso.score(X_test, y_test))
```

    0.7798441902827601
    0.7281132262130203


This model does significnatly better at explaining the variability in the target variable for the test data
73% (lasso) vs 27% (linear)


```python
# Let's plot the predicted vs the real values using the testing data
plt.scatter(pred2, truth, color = "orange")
plt.xlabel('Real Target')
plt.ylabel('Predicted Target')
plt.title("Testing data: Predicted vs Real Target")
plt.xlim(-6, 6)
plt.ylim(-6, 6)

# Add diagonal dashed line
plt.plot([-6, 6], [-6, 6], '--', color='gray')

plt.show()
```


    
![png](../images/Lasso_regression_15_0.png)
    


We can also compare the coefficients for both the linear and the lasso model to show ourselves the feature selection component of lasso regression


```python
# plot size, we want a wide plot since we have so many features
plt.figure(figsize = (15, 5))

# plot for linear model
plt.plot(features,
         linear.coef_,
         linestyle='none', # do not connect the points
         marker='o',
         markersize=7,
         color='mediumvioletred',
         label='Linear Regression')

# plot for lasso regression
plt.plot(lasso.coef_,
         linestyle='none', # do not connect the points
         marker='v',
         markersize=9,
         color='teal',
         label= 'Lasso Regression')

# rotate axis
plt.xticks(rotation = 90, fontsize = 7)
plt.ylabel("Coefficient", fontsize = 12)
plt.legend()
plt.title("Comparison plot of Lasso and Linear regression model")
plt.show()
```


    
![png](../images/Lasso_regression_17_0.png)
    


You can see nicely here how the Lasso regression shrinks many of the coefficients to 0 while keeping the coefficients for a few of the important features, unlike the linear regression, which highlights the benefit of feature selection when using lasso regression

Finally, one last thing I want to show myself is that an alpha = 0 will give us the same/very similar result as linear regression as I read when learning about lasso regression


```python
lasso_zero = Lasso(alpha = 0)
lasso_zero.fit(X_train,y_train)
train_score = lasso_zero.score(X_train,y_train)
test_score = lasso_zero.score(X_test,y_test)

print(test_score)

# Also, if you try this, you'll note that you get a warning: 
## UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
## Warning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.

# We'll compare the coefficients to the linear model below
```

    0.3302251478767664


    /home/coder/micromamba/envs/biotools/lib/python3.12/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
      return fit_method(estimator, *args, **kwargs)
    /home/coder/micromamba/envs/biotools/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.
      model = cd_fast.enet_coordinate_descent(


We can see that using a very small alpha, (alpha = 0) performs much worse than our previous lasso model, with an R^2 of 0.33, showing optimization of the alpha parameter is important


```python
# plot size, we want a wide plot since we have so many features
plt.figure(figsize = (15, 5))

# plot for linear model
plt.plot(features,
         linear.coef_,
         linestyle='none', # do not connect the points
         marker='o',
         markersize=7,
         color='mediumvioletred',
         label='Linear Regression')

# plot for lasso regression with alpha = 0
plt.plot(lasso_zero.coef_,
         linestyle='none', # do not connect the points
         marker='*',
         markersize=9,
         color='purple',
         label= 'Lasso Regression')



# rotate axis
plt.xticks(rotation = 90, fontsize = 7)
plt.ylabel("Coefficient", fontsize = 12)
plt.legend()
plt.title("Comparison plot of Lasso and Linear regression model")
plt.show()
```


    
![png](../images/Lasso_regression_22_0.png)
    


You can see for many of the features, the coefficients are very similar between the linear and lasso regression with alpha = 0.  They're not identifical, but sklearn did warn us that alpha = 0 does not perform well.

We can also prove to ourselves that with a smaller alpha, more features are kept in the model as opposed to previously when we saw so many coefficients pushed to 0

There are many many tutorial and explanations regarding Lasso Regression out there that have much more in-depth explanations than I gave here  
A couple that I found particularly helpful are:
https://www.datacamp.com/tutorial/tutorial-lasso-ridge-regression
https://www.mygreatlearning.com/blog/understanding-of-lasso-regression/

